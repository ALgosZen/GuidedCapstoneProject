{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Trade partition dataset from its temporary location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 5702.548095703125,
      "end_time": 1613494646808.559
     }
    }
   },
   "outputs": [],
   "source": [
    "trade_common = spark.read.parquet(\"/HdiNotebooks/output_dir_csv_json/partition=T/*.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 40.43701171875,
      "end_time": 1613495343645.237
     }
    }
   },
   "source": [
    "### Selecting the Necessary columns for Trade Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 246.621826171875,
      "end_time": 1613494656439.026
     }
    }
   },
   "outputs": [],
   "source": [
    "trade = trade_common.select(\"trade_dt\", \"symbol\", \"exchange\", \"event_tm\",\"event_seq_nb\", \"arrival_tm\", \"trade_pr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 40.390869140625,
      "end_time": 1613495753137.653
     }
    }
   },
   "source": [
    "### Applying Data Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 776.636962890625,
      "end_time": 1613494658939.564
     }
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from datetime import date\n",
    "import pyspark.sql.functions as F \n",
    "\n",
    "\n",
    "# In the exchange dataset, you can uniquely identify a record by the combination of trade_dt,\n",
    "# symbol, exchange, event_tm, event_seq_nb. However, the exchange may correct an error in\n",
    "# any submitted record by sending a new record with the same uniqueID. Such records will come with later arrival_tm. \n",
    "\n",
    "# Below code uses row_number and window partition and orderby to accept records with latest arrival_tme\n",
    "\n",
    "trade_corrected=trade.withColumn(\"row_number\",F.row_number().over(Window.partitionBy(trade.trade_dt,\\\n",
    "                   trade.symbol,trade.exchange,trade.event_tm,trade.event_seq_nb) \\\n",
    "                   .orderBy(trade.arrival_tm.desc()))).filter(F.col(\"row_number\")==1).drop(\"row_number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the Trade Dataset back to Azure Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 13356.571044921875,
      "end_time": 1613494673095.803
     }
    }
   },
   "outputs": [],
   "source": [
    "trade_date = \"2020-08-05\"\n",
    "trade_corrected.coalesce(1).write.parquet(\"wasbs://guidedsparkpro-2021-06-10t15-01-46-464z@guidedsparkprhdistorage.blob.core.windows.net/trade/trade_dt={}\".format(trade_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Quote partition dataset from its temporary location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 751.6689453125,
      "end_time": 1613494879270.603
     }
    }
   },
   "outputs": [],
   "source": [
    "quote_common = spark.read.parquet(\"/HdiNotebooks/output_dir_csv_json/partition=Q/*.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the Necessary columns for Quote Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 238.382080078125,
      "end_time": 1613494942934.174
     }
    }
   },
   "outputs": [],
   "source": [
    "quote=quote_common.select(\"trade_dt\",\"symbol\",\"exchange\",\"event_tm\",\"event_seq_nb\",\"arrival_tm\",\"bid_pr\",\"ask_pr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 246.822998046875,
      "end_time": 1613495115098.263
     }
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from datetime import date\n",
    "import pyspark.sql.functions as F \n",
    "\n",
    "# In the exchange dataset, you can uniquely identify a record by the combination of trade_dt,\n",
    "# symbol, exchange, event_tm, event_seq_nb. However, the exchange may correct an error in\n",
    "# any submitted record by sending a new record with the same uniqueID. Such records will come with later arrival_tm. \n",
    "\n",
    "# Below code uses row_number and window partition and orderby to accept records with latest arrival_tme\n",
    "\n",
    "quote_corrected=quote.withColumn(\"row_number\",F.row_number().over(Window.partitionBy(quote.trade_dt,quote.symbol,\\\n",
    "                                            quote.exchange,quote.event_tm,quote.event_seq_nb).\\\n",
    "                                            orderBy(quote.arrival_tm.desc()))).filter(F.col(\"row_number\")==1).drop(\"row_number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing back the Quote Dataset back to Azure Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_status": {
     "execute_time": {
      "duration": 2260.718017578125,
      "end_time": 1613495123897.714
     }
    }
   },
   "outputs": [],
   "source": [
    "trade_date = \"2020-08-05\"\n",
    "quote.coalesce(1).write.parquet(\"wasbs://guidedsparkpro-2021-06-10t15-01-46-464z@guidedsparkprhdistorage.blob.core.windows.net/quote/trade_dt={}\".format(trade_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
